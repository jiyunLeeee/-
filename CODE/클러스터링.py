# -*- coding: utf-8 -*-
"""클러스터링.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-S0UfkUEARN7v8K_ylp-AQQn-dazUZ9N
"""

# 드라이브 연결
from google.colab import drive
drive.mount('/content/drive')

#라이브러리
import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings('ignore')

car = pd.read_csv('/content/drive/MyDrive/부산 관광 아이디어 공모전/데이터/클러스터링용 데이터/데이터/2022_행정동 단위 차량등록 데이터.csv',encoding='cp949')
safety = pd.read_csv('/content/drive/MyDrive/부산 관광 아이디어 공모전/데이터/클러스터링용 데이터/데이터/부산광역시 행정동별 일단위 유동인구 대비 구급 현황.csv')
bus_info = pd.read_csv('/content/drive/MyDrive/부산 관광 아이디어 공모전/데이터/클러스터링용 데이터/데이터/부산광역시_버스정보안내기 현황.csv',encoding='cp949')
sleep = pd.read_csv('/content/drive/MyDrive/부산 관광 아이디어 공모전/데이터/클러스터링용 데이터/데이터/부산광역시_숙박시설 주변 시설정보_202211.CSV')
move_pop = pd.read_csv('/content/drive/MyDrive/부산 관광 아이디어 공모전/데이터/클러스터링용 데이터/데이터/부산광역시_시간대_행정동별_인구이동.csv',encoding='cp949')
sleep_good = pd.read_csv('/content/drive/MyDrive/부산 관광 아이디어 공모전/데이터/클러스터링용 데이터/데이터/부산광역시_우수숙박업소현황_202106.csv')
pop = pd.read_csv('/content/drive/MyDrive/부산 관광 아이디어 공모전/데이터/클러스터링용 데이터/데이터/부산주민등록인구.csv') # row 205
tour = pd.read_excel('/content/drive/MyDrive/부산 관광 아이디어 공모전/데이터/클러스터링용 데이터/데이터/부산지역 관광시설분야 공공데이터(열린관광시설정보)_수정.xlsx')
food = pd.read_excel('/content/drive/MyDrive/부산 관광 아이디어 공모전/데이터/클러스터링용 데이터/데이터/부산지역 관광식음료분야 공공데이터(열린관광시설정보)_수정.xlsx')
bus_num = pd.read_csv('/content/drive/MyDrive/부산 관광 아이디어 공모전/데이터/클러스터링용 데이터/데이터/행정동별 버스 정류소 개수.csv',encoding='cp949')
wifi =  pd.read_csv('/content/drive/MyDrive/부산 관광 아이디어 공모전/데이터/클러스터링용 데이터/데이터/무료와이파이정보.csv')

"""### **전처리**

#### *2022_행정동 단위 차량등록*
"""

car_fuel= car.groupby(['adn_dgnm','vehi_fuel_nm']).count()
car_sum = car.groupby(['adn_dgnm']).sum().reset_index(inplace=False)
car_sum = car_sum.loc[:,['adn_dgnm','vehi_num']]
car_sum.columns = ['행정동','자동차수']

car_sum.행정동 = car_sum.행정동.str.replace('가야제1동','가야1동')
car_sum.행정동 = car_sum.행정동.str.replace('당감제1동','당감1동')

"""#### *부산광역시 행정동별 일단위 유동인구 대비 구급 현황* (해결)

"""

safety = pd.read_csv('/content/drive/MyDrive/부산 관광 아이디어 공모전/데이터/클러스터링용 데이터/데이터/부산광역시 행정동별 일단위 유동인구 대비 구급 현황.csv')

safety.columns = ['행정동','신고일자','구급출동건수','환자발생유형질병건수','환자발생유형질병외건수','환자발생유형기타건수',
                  '중증외상건수','심정지건수','교통사고건수','환자성별남성건수','환자성별여성건수','환자나이10세미만',
                  '환자나이10~19','환자나이20~29','환자나이30~39','환자나이40~49','환자나이50~59','환자나이60세이상',
                  '교통정체건수','폭우건수','폭설건수','평균현장거리','평균출동소요시간','버스승객수','지하철승객수']

safety.info()

# 평균출동소요시간 초단위로 바꾸기
time = safety['평균출동소요시간'].str.split(':')
time['평균출동소요시간(분)'] = time.str.get(1).astype('float')
time['평균출동소요시간(초)'] = time.str.get(2).astype('float')

safety['평균출동소요시간(초)'] = time['평균출동소요시간(분)']*60 + time['평균출동소요시간(초)']

#시간나면, 상관관계 플랏을 통해 변수 제거해도 좋을 듯
safety = safety[['행정동','구급출동건수','중증외상건수','심정지건수','교통사고건수',
                 '교통정체건수','폭우건수','폭설건수','평균출동소요시간(초)']]
safety

# NA 처리
safety.isna().sum() # 평균출동소요시간(초): 474
safety['평균출동소요시간(초)']=safety.groupby('행정동')['평균출동소요시간(초)'].apply(lambda x: x.fillna(x.mean()))

saftey_mean = safety.groupby(['행정동']).mean().reset_index(inplace=False)
saftey_mean

np.setdiff1d(saftey_mean.행정동, data.행정동)

"""#### *부산광역시_버스정보안내기 현황*  #이거 수정해야함

"""

bus_info = pd.read_csv('/content/drive/MyDrive/부산 관광 아이디어 공모전/데이터/클러스터링용 데이터/데이터/부산광역시_버스정보안내기 현황.csv',encoding='cp949')
bus_info

bus_info['주소']=bus_info.주소.str.replace('부산','')

address_change = bus_info['주소']
address_change

import json
import requests
from tqdm import tqdm

locations = []

for i in tqdm(address_change):
    url = 'https://dapi.kakao.com/v2/local/search/address.json?query={}'.format(i)
    headers = {"Authorization": "KakaoAK 1a7195a14d786682aecda5217e9396d1"}
    place = requests.get(url, headers = headers).json()['documents']
    locations.append(place)

city = [] ## 시, 군
town = [] ## 동, 읍, 면

for i in tqdm(range(0,len(locations))):

    try:
        city.append(locations[i][0].get('address').get('region_2depth_name'))
        town.append(locations[i][0].get('address').get('region_3depth_name'))

    except IndexError:
        print(i,'번째 주소 못가져옴', end ='')
        print()
        city.append('없음')
        town.append('없음')

    except AttributeError:
        city.append(locations[i][0].get('road_address').get('region_2depth_name'))
        town.append(locations[i][0].get('road_address').get('region_3depth_name'))

city_town = np.array([city,town]).T
df_temp = pd.DataFrame(city_town, columns = ['region_2depth_name','region_3depth_name'])
df_temp.head()



bus_info = bus_info[['동','행정동','위치명(정류장명)']]
bus_info_count = bus_info.groupby(['동','행정동']).count().reset_index(inplace=False)
bus_info_count.rename(columns={'위치명(정류장명)':'정류장 개수'},inplace=True)
bus_info_count

"""#### *부산광역시_숙박시설 주변 시설정보_202211* -> 이거 크롤링 해야할 듯

"""

# sleep = pd.read_csv('/content/drive/MyDrive/부산 관광 아이디어 공모전/데이터/클러스터링용 데이터/데이터/부산광역시_숙박시설 주변 시설정보_202211.CSV')
sleep.columns = ['숙박업명','숙박업주소','시설명','시설주소','시설위도','시설경도','분류내용','검색량합계값']
sleep

address = sleep['숙박업주소'].str.split(' ')
sleep['동'] = address.str.get(1)
sleep['행정동'] = address.str.get(2)

sleep = sleep[['동','행정동','숙박업명','시설명','검색량합계값']]

sleep.isna().sum()
sleep['행정동'].nunique()

# 시간 남으면 시각화 하자...
sleep

sleep.groupby('행정동').count().reset_index(inplace=False)

"""#### *시간대 행정동별 인구이동*

"""

move_pop = pd.read_csv('/content/drive/MyDrive/부산 관광 아이디어 공모전/데이터/클러스터링용 데이터/데이터/부산광역시_시간대_행정동별_인구이동.csv',encoding='cp949')

move_pop_mean = move_pop.groupby('행정동명').mean().round(3).reset_index()
move_pop_mean = move_pop_mean[['행정동명','이동건수']]
move_pop_mean.rename(columns={'이동건수':'유동인구(평균)','행정동명':'행정동'},inplace=True)

move_pop_mean.행정동 = move_pop_mean.행정동.str.replace('일광면','일광읍')

"""#### *우수숙박업소* --> unique 값 16개 존재 나중에 참고요으로 써야할 듯 (EDA용으로)"""

sleep_good = pd.read_csv('/content/drive/MyDrive/부산 관광 아이디어 공모전/데이터/클러스터링용 데이터/데이터/부산광역시_우수숙박업소현황_202106.csv')
sleep_good

sleep_good['GUGUN_NM'].nunique()

"""#### *주민등록인구*"""

pop = pd.read_csv('/content/drive/MyDrive/부산 관광 아이디어 공모전/데이터/클러스터링용 데이터/데이터/부산주민등록인구.csv') # row 205
pop

pop.읍면동명 = pop.읍면동명.str.replace('제','')
pop.읍면동명 = pop.읍면동명.str.replace('거','거제')

pop = pop[['시도명','읍면동명','계','남자','여자']]
pop.rename(columns={'읍면동명':'행정동'},inplace=True)

"""#### 급격한 면적처리"""

land = pd.read_csv('/content/drive/MyDrive/부산 관광 아이디어 공모전/(쓱싹) 모델링/이지윤/면적.csv')
land

land = land[['ADM_DR_CD','ADM_DR_NM','면적(평방 미터)']]
land['ADM_DR_CD']= land['ADM_DR_CD'].astype('str')

land = land.loc[land['ADM_DR_CD'].str[0:2]=='21']  #부산 번호가 21

land.행정동 = land.행정동.str.replace('일광면','일광읍')

land.columns = ['행정동 번호','행정동','면적(평방미터)']
land.to_csv('행정동 면적(평방미터).csv',index=False)



"""#### 와이파이"""

wifi =  pd.read_csv('/content/drive/MyDrive/부산 관광 아이디어 공모전/데이터/클러스터링용 데이터/데이터/무료와이파이정보.csv')
wifi

wifi.isna().sum() #없음

address = wifi['소재지도로명주소'].str.split(' ')
wifi['행정동'] = address.str.get(2)

"""####

#### 문화영화광업소
"""

movie = pd.read_csv('/content/drive/MyDrive/부산 관광 아이디어 공모전/데이터/클러스터링용 데이터/데이터/문화_영화업소/fulldata_03_13_02_P_영화상영관.csv',encoding='cp949')
movie.columns

movie.도로명전체주소.unique()

"""### 최종 데이터셋 만들기"""

# data = pd.merge(land,pop,how='left',on='행정동')
# data= pd.merge(data,move_pop_mean,how='left',on='행정동')
# data

data = pd.read_csv('/content/drive/MyDrive/부산 관광 아이디어 공모전/(쓱싹) 모델링/이지윤/클러스터링1차데이터.csv')
data

data['유동인구/총인구'] = data['유동인구(평균)']/(data['유동인구(평균)']+ data['계'])
data['면적당 인구'] = data['계']/data['면적(평방미터)']/1000000
data['면적당 평균소요시간'] = data['평균출동소요시간(초)']/data['면적(평방미터)']/1000000

data= data[['행정동','구급출동건수','교통사고건수', '교통정체건수','유동인구/총인구','면적당 인구','면적당 평균소요시간']]
data

data.corr(method='pearson')

"""### 클러스터링"""

from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
from yellowbrick.cluster import SilhouetteVisualizer

cluster_data = data[['교통정체건수','교통사고건수','유동인구/총인구','면적당 평균소요시간']]

cluster_data.corr(method='pearson')

# 스케일링
scaler = MinMaxScaler()
cluster_data_scaled = scaler.fit_transform(cluster_data)

"""#### kmeans"""

def elbow(data):
  sse=[]
  for i in range(1,11):
    km = KMeans(n_clusters=i, init='k-means++', random_state=119) # 해당 부분 함수를 다른 클러스터링 함수로 바꿔서 써도 됨!
    km.fit(data)
    sse.append(km.inertia_)
  plt.plot(range(1,11),sse,marker='o')
  plt.xlabel('클러스터 개수')
  plt.ylabel('SSE')
  plt.show()

elbow(cluster_data_scaled)

def plot_Silhouette(data,k, text):
  for i in range(2,k):
      km = KMeans(n_clusters=i, init='k-means++', random_state=119) # 해당 부분 함수를 다른 클러스터링 함수로 바꿔서 써도 됨!
      km.fit(data)

      fig = plt.figure()
      fig.set_size_inches(15, 5)
      visualizer = SilhouetteVisualizer(km, colors='yellowbrick')
      visualizer.fit(data)

      visualizer.ax.set_title(i,fontsize=15, weight='bold')
      visualizer.ax.set_xlabel("Silhouette Value",fontsize=15, weight='bold')
      visualizer.ax.set_ylabel("군집",fontsize=15, weight='bold')
      visualizer.ax.set_yticks([])

      plt.show()

# 이게 맞나 성능 뭐지
plot_Silhouette(cluster_data_scaled,8, 'text')

"""#### GMM"""

# 안됨
from sklearn.mixture import GaussianMixture

gmm = GaussianMixture(n_components = 5, random_state = 0)
gmm.fit(cluster_data_scaled.data)
gmm_cluster_labels = gmm.predict(cluster_data_scaled)

cluster_data_scaled['gmm_cluster'] = gmm_cluster_labels
cluster_data_scaled['target'] = data['행정동']

result = cluster_data_scaled.groupby(['target'])['gmm_cluster'].value_counts()
print(result)

"""#### Heirachial clustering"""

from scipy.cluster.hierarchy import linkage, dendrogram
from sklearn.cluster import AgglomerativeClustering

linkage_list = ['single', 'complete', 'average', 'centroid', 'ward']
data = [cluster_data_scaled, cluster_data]

fig, axes = plt.subplots(nrows=len(linkage_list), ncols=2, figsize=(16, 35))
for i in range(len(linkage_list)):
    for j in range(len(data)):
        hierarchical_single = linkage(data[j], method=linkage_list[i])
        dn = dendrogram(hierarchical_single, ax=axes[i][j])
        axes[i][j].title.set_text(linkage_list[i])
plt.show()

# 다시 fitting
agg_clustering = AgglomerativeClustering(n_clusters=3, linkage='ward')
labels = agg_clustering.fit_predict(cluster_data)
cluster_data['cluster'] = labels
cluster_data.groupby('cluster').size()

from sklearn.metrics import silhouette_samples, silhouette_score
score=silhouette_score(cluster_data,cluster_data['cluster'])
score